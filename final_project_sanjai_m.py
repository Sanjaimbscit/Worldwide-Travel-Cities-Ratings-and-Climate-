# -*- coding: utf-8 -*-
"""Final Project-SANJAI.M

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1UvzMjHAPBsmNmUFH60CP_ZuiBP696bLi

#PROJECT TITLE

---
#**Worldwide Travel Cities (Ratings and Climate)** :
---

#Domain

Travel & Tourism Analytics

#Objective
To develop a data-driven travel recommendation and analysis system that helps users discover and compare global cities based on their preferences in themes (e.g., culture, adventure, cuisine), travel budget, ideal trip duration, and seasonal climate conditions — enabling personalized, efficient, and enjoyable trip planning experiences.

# Dataset information
Source: Kaggle

Time Range: 2010 – 2025

Total Cities: 560+

#Type of problem

>Classification Problem
Goal: Predict categorical outcomes based on city features

>Target Variables:

>budget_level → (Budget, Mid-range, Luxury)


>Algorithms Used:

>Random Forest Classifier


>Logistic Regression

#STAGE 1

## Inital EDA
"""

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns


from sklearn.model_selection import train_test_split
from sklearn.linear_model import LogisticRegression
from sklearn.ensemble import RandomForestClassifier, RandomForestRegressor

from sklearn.svm import SVC
from sklearn.neighbors import KNeighborsClassifier


from sklearn.metrics import classification_report, accuracy_score, mean_squared_error, r2_score
from sklearn.preprocessing import LabelEncoder, MinMaxScaler

URL='https://raw.githubusercontent.com/Sanjaimbscit/Worldwide-Travel-Cities-Ratings-and-Climate-/refs/heads/main/Worldwide%20Travel%20Cities%20Dataset%20(Ratings%20and%20Climate).csv'

df = pd.read_csv(URL)
df

df.info()

df.describe()

df.columns

"""# STAGE 2

## EDA (Visualization) and Pre-processing

## Handling Missing Values
"""

missing_value = df.isnull().sum()
missing_value

top_countries = df['country'].value_counts().head(10).reset_index()
top_countries.columns = ['Country', 'Count']
print(top_countries)

"""## Duplicated"""

df.duplicated().sum()

"""## Outlier Detection"""

df.shape

import seaborn as sns
import matplotlib.pyplot as plt

sns.boxplot(x=df['cuisine'], color='lightblue')
plt.title('Boxplot of Cuisine Rating')
plt.xlabel('Cuisine Rating (0–5)')
plt.tight_layout()
plt.show()

# === Drop Duplicate Records ===
df = df.drop_duplicates()

# === Outlier Treatment Using IQR Method ===
numeric_cols = df.select_dtypes(include=[np.number]).columns  # Select only numeric columns

for col in numeric_cols:
    Q1 = df[col].quantile(0.25)
    Q3 = df[col].quantile(0.75)
    IQR = Q3 - Q1
    lower = Q1 - 1.5 * IQR
    upper = Q3 + 1.5 * IQR

    # Cap the outliers
    df[col] = np.where(df[col] < lower, lower,
                       np.where(df[col] > upper, upper, df[col]))

print(" Duplicates removed and outliers treated using IQR method.")

import matplotlib.pyplot as plt
import seaborn as sns

# Select numeric columns (theme scores + others)
numeric_cols = df.select_dtypes(include=[np.number]).columns.tolist()

# Set up features and target
X = df[numeric_cols].drop(columns=['cuisine'])  # Example: predicting cuisine
y = df['cuisine']

# === Boxplots for feature variables ===
plt.figure(figsize=(20, 15))
for i, col in enumerate(X.columns):
    plt.subplot(4, 3, i + 1)
    sns.boxplot(y=X[col], color='skyblue')
    plt.title(f'Boxplot - {col}')
plt.tight_layout()
plt.show()

# === Boxplot for target variable ===
plt.figure(figsize=(6, 4))
sns.boxplot(y=y, color='salmon')
plt.title("Boxplot - Cuisine Rating")
plt.ylabel("Cuisine (0–5 scale)")
plt.show()

df.shape

"""# Skewness"""

skewness = df['cuisine'].skew()
print("Skewness of Cuisine Rating:", skewness)

from numpy import log1p, sqrt
import numpy as np

df_transformed = df.copy()
numeric_cols = df_transformed.select_dtypes(include=[np.number]).columns

for col in numeric_cols:
    skew_val = df_transformed[col].skew()

    try:
        if skew_val < -0.5 or skew_val > 0.5:
            if (df_transformed[col] <= 0).any():
                # For negative/zero values, use sqrt after shifting
                min_val = df_transformed[col].min()
                df_transformed[col] = sqrt(df_transformed[col] - min_val + 1)
                print(f"√ Square Root Transform applied to: {col}")
            else:
                # Use log1p when all values > 0
                df_transformed[col] = log1p(df_transformed[col])
                print(f"log1p Transform applied to: {col}")
        else:
            print(f"No transform needed for: {col} (Skew={round(skew_val, 2)})")
    except Exception as e:
        print(f"Error transforming {col}: {e}")

"""## EDA after preprocessing"""

df.info()

df.describe()

df.shape

"""##Visualizations

## Univariate Analysis
"""

X.hist(bins=30, figsize=(18, 12), color='teal')
plt.suptitle("Univariate Distribution - Features")
plt.show()

"""## Bivariate Analysis"""

import matplotlib.pyplot as plt
import seaborn as sns

plt.figure(figsize=(20, 15))

for i, col in enumerate(X.columns):
    plt.subplot(5, 3, i + 1)
    sns.scatterplot(x=X[col], y=y)
    plt.title(f'{col} vs Cuisine Rating')  # Adjust title to match your target
    plt.xlabel(col)
    plt.ylabel('Cuisine')

plt.tight_layout()
plt.show()

"""## Multivariate Analysis"""

import seaborn as sns
import matplotlib.pyplot as plt

# Select subset of features + target
subset = pd.concat([X[['culture', 'nightlife', 'urban']], y], axis=1)

# Pairplot
sns.pairplot(subset, diag_kind='kde')
plt.suptitle("Multivariate Analysis - Selected Features + Target", y=1.02)
plt.show()

import numpy as np

# Skewness before log transformation
print("Skewness before log:\n", X.skew().sort_values(ascending=False))

# Apply log1p transformation to features
X_log = X.copy()
for col in X.columns:
    X_log[col] = np.log1p(X[col])

# Apply log1p to target
y_log = np.log1p(y)

# Skewness after transformation
print("\nSkewness after log:\n", X_log.skew().sort_values(ascending=False))

# === Features Summary ===
summary_features = X_log.describe().T
print(" Feature Summary (Log-Transformed Features):\n")
display(summary_features)

# === Target Summary ===
summary_target = y_log.describe()
print("\n Target Summary (Log-Transformed Target):\n")
print(summary_target)

# Create interaction term between culture and nightlife
X['culture_nightlife_interact'] = X['culture'] * X['nightlife']

# Plot interaction vs target (e.g., cuisine)
sns.scatterplot(x=X['culture_nightlife_interact'], y=y)
plt.title("Interaction: Culture × Nightlife vs Cuisine Rating")
plt.xlabel("Culture × Nightlife")
plt.ylabel("Cuisine")
plt.tight_layout()
plt.show()

"""# STAGE 3

## Feature Engineering
"""

print(df.columns.tolist())

X = df.drop(columns=['cuisine'])  # or any other target

# Use original DataFrame to ensure all columns are available
df['culture_nightlife'] = df['culture'] * df['nightlife']
df['nature_to_urban'] = df['nature'] / (df['urban'] + 1e-5)

theme_cols = ['culture', 'adventure', 'nature', 'beaches', 'nightlife', 'cuisine', 'wellness', 'urban', 'seclusion']
df['total_theme_score'] = df[theme_cols].sum(axis=1)

df['trip_intensity'] = pd.cut(df['total_theme_score'],
                              bins=[0, 10, 20, 30, 45],
                              labels=['Low', 'Medium', 'High', 'Extreme'])

from sklearn.preprocessing import LabelEncoder
le = LabelEncoder()
df['trip_intensity_encoded'] = le.fit_transform(df['trip_intensity'].astype(str))

# Preview the new features
print(df[['culture_nightlife', 'nature_to_urban', 'total_theme_score', 'trip_intensity', 'trip_intensity_encoded']].head())

"""## Feature Selection"""

# Example: Use 'budget_level' as a proxy for income group
X['income_group'] = X['budget_level']  # e.g., 'Budget', 'Mid-range', 'Luxury'

from sklearn.preprocessing import LabelEncoder

le = LabelEncoder()
X['income_group_encoded'] = le.fit_transform(X['income_group'].astype(str))

# View mapping
mapping = dict(zip(le.classes_, le.transform(le.classes_)))
print("Encoded Income Groups:", mapping)

from sklearn.preprocessing import MinMaxScaler
from sklearn.feature_selection import SelectKBest, chi2, f_classif
import pandas as pd

#  Target Variable (you can change this)
target_col = 'cuisine'
y = df[target_col]

# 1. Prepare Only Numeric Features (drop non-numeric like strings or categorical)
X_numeric = df.select_dtypes(include=['int64', 'float64'])

# Drop constant columns
X_numeric = X_numeric.loc[:, X_numeric.nunique() > 1]

# -------------------------
#  CORRELATION MATRIX
# -------------------------
df_corr = X_numeric.copy()
df_corr['target'] = y

correlation = df_corr.corr()['target'].abs().sort_values(ascending=False)
selected_corr_features = correlation[correlation > 0.2].index.tolist()
selected_corr_features.remove('target')
X_corr_selected = X_numeric[selected_corr_features]

print(" Selected Features (Correlation > 0.2):")
print(selected_corr_features)

# -------------------------
#  CHI² FEATURE SELECTION
# -------------------------
X_scaled = pd.DataFrame(MinMaxScaler().fit_transform(X_numeric), columns=X_numeric.columns)

# Convert numeric target into categories for Chi²
y_class = pd.cut(y, bins=[-float('inf'), 2, 4, float('inf')], labels=[0, 1, 2])

chi2_selector = SelectKBest(score_func=chi2, k=10)
X_chi2_selected = chi2_selector.fit_transform(X_scaled, y_class)
chi2_features = X_scaled.columns[chi2_selector.get_support()].tolist()

print("\n Selected Features (Chi²):")
print(chi2_features)

# -------------------------
#  ANOVA F-TEST
# -------------------------
anova_selector = SelectKBest(score_func=f_classif, k=10)
X_anova_selected = anova_selector.fit_transform(X_scaled, y_class)
anova_features = X_scaled.columns[anova_selector.get_support()].tolist()

print("\n Selected Features (ANOVA):")
print(anova_features)

"""## Train-Test Split"""

classification_target = 'budget_level'

if classification_target not in df.columns:
    print(f"Error: Target column '{classification_target}' not found in the DataFrame.")
else:

    X_classification = df.select_dtypes(include=[np.number]).drop(columns=[col for col in df.select_dtypes(include=[np.number]).columns if col in ['culture_nightlife_interact', 'nature_to_urban', 'total_theme_score', 'trip_intensity_encoded'] or col == 'cuisine']) # Exclude columns potentially created during Feature Engineering
    # Include the engineered numeric features if they were created in df and are not the target
    engineered_numeric_cols = ['culture_nightlife', 'nature_to_urban', 'total_theme_score', 'trip_intensity_encoded']
    for col in engineered_numeric_cols:
        if col in df.columns and col not in X_classification.columns:
            if pd.api.types.is_numeric_dtype(df[col]):
                X_classification[col] = df[col]

    y_classification = df[classification_target]

X_classification = X_classification.fillna(X_classification.mean())

le_classification = LabelEncoder()
y_classification_encoded = le_classification.fit_transform(y_classification)

scaler_classification = MinMaxScaler()
X_classification_scaled = scaler_classification.fit_transform(X_classification)

# Split data for classification
X_train_clf, X_test_clf, y_train_clf, y_test_clf = train_test_split(
X_classification_scaled, y_classification_encoded, test_size=0.2, random_state=42, stratify=y_classification_encoded)

print("\n--- Classification Model Training ---")

"""#Model building"""

# Train Classification Model - RandomForestClassifier
classifier = RandomForestClassifier(n_estimators=100, random_state=42)
# Train Classification Model - Svm
classifier.fit(X_train_clf, y_train_clf)
# Train Classification Model - LogisticRegression
log_reg = LogisticRegression(max_iter=1000, random_state=42) # Increased max_iter for convergence
log_reg.fit(X_train_clf, y_train_clf)


# Train Classification Model - Support Vector Machine (SVM)
svm_model = SVC(probability=True, random_state=42)  # probability=True allows for probability predictions if needed
svm_model.fit(X_train_clf, y_train_clf)

# Train Classification Model - K-Nearest Neighbors (KNN)
knn_model = KNeighborsClassifier(n_neighbors=5)  # You can tune n_neighbors
knn_model.fit(X_train_clf, y_train_clf)

#  predictions
y_pred_clf = classifier.predict(X_test_clf)
y_pred_svm = svm_model.predict(X_test_clf)
y_pred_log_reg = log_reg.predict(X_test_clf)
y_pred_knn = knn_model.predict(X_test_clf)

# Evaluate classification model
accuracy_clf = accuracy_score(y_test_clf, y_pred_clf)
report_clf = classification_report(y_test_clf, y_pred_clf, target_names=le_classification.classes_)


print(f"Classification Accuracy: {accuracy_clf:.4f}")
print("Classification Report:\n", report_clf)

accuracy_svm = accuracy_score(y_test_clf, y_pred_svm)
report_svm = classification_report(y_test_clf, y_pred_svm, target_names=le_classification.classes_)

print("\n--- Support Vector Machine (SVM) ---")
print(f"SVM Accuracy: {accuracy_svm:.4f}")

# Evaluate Logistic Regression model
accuracy_log_reg = accuracy_score(y_test_clf, y_pred_log_reg)
report_log_reg = classification_report(y_test_clf, y_pred_log_reg, target_names=le_classification.classes_)


print(f"Logistic Regression Accuracy: {accuracy_log_reg:.4f}")
print("Logistic Regression Report:\n", report_log_reg)


# Evaluate KNN model
accuracy_knn = accuracy_score(y_test_clf, y_pred_knn)
report_knn = classification_report(y_test_clf, y_pred_knn, target_names=le_classification.classes_)

print("\n--- K-Nearest Neighbors (KNN) ---")
print(f"KNN Accuracy: {accuracy_knn:.4f}")
print("KNN Report:\n", report_knn)

print("\n--- Comparing Models ---")
print("Random Forest Classifier:")
print(f"  Accuracy: {accuracy_clf:.4f}")
print("  Report:\n", report_clf)

print("\nLogistic Regression:")
print(f"  Accuracy: {accuracy_log_reg:.4f}")
print("  Report:\n", report_log_reg)

print("\nSupport Vector Machine (SVM):")
print(f"  Accuracy: {accuracy_svm:.4f}")
print("  Report:\n", report_svm)

print("\nK-Nearest Neighbors (KNN):")
print(f"  Accuracy: {accuracy_knn:.4f}")
print("  Report:\n", report_knn)

print("")

# Simple comparison based on accuracy
if accuracy_clf > accuracy_log_reg:
  print("\nRandom Forest Classifier performed better in terms of accuracy.")
elif accuracy_log_reg > accuracy_clf:
  print("\nLogistic Regression performed better in terms of accuracy.")
else:
  print("\nBoth models performed similarly in terms of accuracy.")

results = {
    'Random Forest': accuracy_clf,
    'Logistic Regression': accuracy_log_reg,
    'SVM': accuracy_svm,
    'KNN': accuracy_knn
}

# best performing model based on accuracy
best_model_name = max(results, key=results.get)
print(f"\nBased on accuracy, the best performing model is: {best_model_name} ({results[best_model_name]:.4f})")

"""#Documentation


## 1. Project Title and Domain

- **PROJECT TITLE:** Worldwide Travel Cities (Ratings and Climate)
- **Domain:** Travel & Tourism Analytics

## 2. Objective

The primary objective is to create a data-driven travel recommendation and analysis system. This system will assist users in discovering and comparing global cities by considering various factors:

- Preferences in themes (culture, adventure, cuisine, etc.)
- Travel budget
- Ideal trip duration
- Seasonal climate conditions

The goal is to facilitate personalized, efficient, and enjoyable trip planning.

## 3. Dataset Information

- **Source:** Kaggle
- **Time Range:** 2010 – 2025
- **Total Cities:** 560+

## 4. Type of Problem

- **Classification Problem:** The project focuses on predicting categorical outcomes based on city features.
- **Target Variable:** `budget_level` (which can be 'Budget', 'Mid-range', or 'Luxury').
- **Algorithms Used:**
    - Random Forest Classifier
    - Logistic Regression
    - Support Vector Machine (SVM)
    - K-Nearest Neighbors (KNN)

## 5. Workflow Stages

### STAGE 1: Initial EDA (Exploratory Data Analysis)

- Load the dataset from the provided URL using pandas.
- Display the first few rows of the DataFrame (`df.head()`).
- Get a concise summary of the DataFrame, including data types and non-null values (`df.info()`).
- Generate descriptive statistics for numerical columns (`df.describe()`).
- List the column names (`df.columns`).

### STAGE 2: EDA (Visualization) and Pre-processing

#### Handling Missing Values

- Calculate the sum of missing values for each column (`df.isnull().sum()`).
- Identify and print the top 10 countries based on city count.

#### Handling Duplicates

- Check for duplicate rows in the dataset (`df.duplicated().sum()`).
- Remove duplicate rows (`df = df.drop_duplicates()`).

#### Outlier Detection and Treatment

- Visualize outliers using boxplots for selected columns (e.g., 'cuisine').
- Identify numerical columns.
- Apply the Interquartile Range (IQR) method to detect and treat outliers by capping values at the lower and upper bounds (`Q1 - 1.5*IQR`, `Q3 + 1.5*IQR`).
- Print a confirmation message after removing duplicates and treating outliers.
- Re-visualize boxplots for features and the target variable ('cuisine' in the example) after outlier treatment to observe the changes.

#### Skewness

- Calculate the skewness of the target variable (e.g., 'cuisine').
- Apply transformations (log1p or square root) to numerical columns with significant skewness (> 0.5 or < -0.5) to make distributions more symmetrical. Use log1p for positive values and square root (with shifting for non-positive) otherwise.

#### EDA after Preprocessing

- Re-examine `df.info()` and `df.describe()` to see the changes after preprocessing steps.
- Check the DataFrame shape (`df.shape`).

#### Visualizations

- **Univariate Analysis:** Create histograms for numerical feature variables to understand their distributions.
- **Bivariate Analysis:** Create scatter plots to visualize the relationship between each feature and the target variable (e.g., 'cuisine').
- **Multivariate Analysis:** Use pairplots for a subset of features and the target to visualize relationships and distributions across multiple variables.
- Analyze skewness before and after applying log transformation to observe the effect on distributions.
- Generate summary statistics for log-transformed features and the target.
- Create interaction terms between relevant features (e.g., 'culture' and 'nightlife').
- Visualize the relationship between the interaction term and the target variable.

### STAGE 3: Feature Engineering

- List the columns in the DataFrame.
- Create new features based on domain knowledge or combinations of existing features:
    - `culture_nightlife`: Interaction term (product of culture and nightlife scores).
    - `nature_to_urban`: Ratio of nature to urban scores.
    - `total_theme_score`: Sum of all theme scores.
    - `trip_intensity`: Categorical feature based on `total_theme_score` (Low, Medium, High, Extreme) created using `pd.cut`.
- Encode the `trip_intensity` categorical feature into numerical format using `LabelEncoder`.
- Preview the newly created features.

### STAGE 4: Feature Selection

- Select a target variable (e.g., 'cuisine' for demonstration, but the main target is `budget_level`).
- Prepare only numeric features by dropping non-numeric and constant columns.
- **Correlation Matrix:** Calculate the correlation matrix between numerical features and the target. Select features with absolute correlation greater than a specified threshold (e.g., 0.2).
- **Chi² Feature Selection:**
    - Scale numeric features using `MinMaxScaler`.
    - Convert the target variable into categories suitable for Chi² (e.g., by binning).
    - Use `SelectKBest` with the `chi2` score function to select the top k features.
- **ANOVA F-Test:**
    - Use `SelectKBest` with the `f_classif` score function to select the top k features.
- Print the lists of features selected by each method.

### STAGE 5: Model Building (Classification)

- Define the classification target variable (`budget_level`).
- Prepare features (`X_classification`) by selecting numerical columns from the DataFrame, ensuring engineered features are included and the target is excluded. Handle potential missing values by filling with the mean.
- Prepare the target variable (`y_classification`).
- Encode the categorical target variable into numerical format using `LabelEncoder`.
- Scale the features using `MinMaxScaler`.
- Split the data into training and testing sets (`X_train_clf`, `X_test_clf`, `y_train_clf`, `y_test_clf`) using `train_test_split`, ensuring stratification based on the target variable.

#### Train and Evaluate Models

- **Random Forest Classifier:**
    - Initialize and train a `RandomForestClassifier` model.
    - Make predictions on the test set.
    - Evaluate the model using `accuracy_score` and `classification_report`. Print the results.
- **Logistic Regression:**
    - Initialize and train a `LogisticRegression` model (adjust `max_iter` if needed for convergence).
    - Make predictions on the test set.
    - Evaluate the model using `accuracy_score` and `classification_report`. Print the results.
- **Support Vector Machine (SVM):**
    - Initialize and train an `SVC` model (set `probability=True` if needed).
    - Make predictions on the test set.
    - Evaluate the model using `accuracy_score` and `classification_report`. Print the results.
- **K-Nearest Neighbors (KNN):**
    - Initialize and train a `KNeighborsClassifier` model (choose `n_neighbors`).
    - Make predictions on the test set.
    - Evaluate the model using `accuracy_score` and `classification_report`. Print the results.

#### Model Comparison

- Print a comprehensive comparison of the accuracy scores for all trained classification models (Random Forest, Logistic Regression, SVM, KNN).
- Identify and print the model with the highest accuracy based on the test set evaluation.

"""

print(f"Overall Classification Accuracy: {accuracy_clf*100:.2f}%")
print(f"Overall Logistic Regression Accuracy: {accuracy_log_reg*100:.2f}%")
print(f"Overall SVM Accuracy: {accuracy_svm*100:.2f}%")
print(f"Overall KNN Accuracy: {accuracy_knn*100:.2f}%")

"""# Future Enhancements



# 1.  **Implement User Input Interface:**
     -   Develop a simple interface (e.g., using `ipywidgets` in Colab/Jupyter) to take user preferences as input.
     -   Inputs could include: desired themes (culture, adventure, etc.), budget level ('Budget', 'Mid-range', 'Luxury'), ideal trip duration range, and preferred season/climate.

 2.  **Integrate Recommendation Logic:**
     -   Based on user inputs, filter the DataFrame to find cities that match the criteria.
     -   Use the trained classification model (`classifier` for `budget_level`) to predict the budget level for cities if it's not explicitly provided or to validate matches.
     -   Consider using theme scores and climate data to rank or select the best-matching cities from the filtered list.

 3.  **Clustering for Similar Cities:**
     -   Apply clustering algorithms (e.g., K-Means) to group cities based on their features (theme scores, climate, etc.).
     -   When a user is interested in a specific city, recommend other cities within the same cluster.

 4.  **Advanced Recommendation System:**
     -   Explore more sophisticated recommendation techniques like Collaborative Filtering (if user-city interaction data were available) or Content-Based Filtering using city features.

 5.  **Visualization Enhancement:**
     -   Create interactive visualizations (e.g., using Plotly or Bokeh) to display recommended cities on a map, compare their features, or show climate trends.

 6.  **Model Improvement:**
     -   Tune hyperparameters of the trained classification models using techniques like GridSearchCV or RandomizedSearchCV.
     -   Experiment with other classification algorithms relevant to multi-class problems (e.g., Gradient Boosting, Naive Bayes).
     -   Investigate feature importance from models like RandomForestClassifier to understand which features are most influential in predicting budget level.

 7.  **Climate Matching:**
     -   Refine the climate matching logic to consider seasonal averages and variance, allowing users to specify preferred temperature ranges or conditions.

 8.  **Add External Data:**
     -   Integrate external data sources, such as real-time flight/hotel prices, local events calendars, or safety ratings, to enrich recommendations.

 9.  **Regression Task (Predicting Ratings):**
     -   Expand the project to include regression models (like the `RandomForestRegressor` example added above) to predict specific ratings (like 'cuisine', 'nature', etc.) based on other city features. This could be useful for understanding city characteristics.

 10. **Create a Web Application:**
      - Deploy the recommendation system as a simple web application using frameworks like Flask or Django, allowing users to interact with it through a browser.
"""